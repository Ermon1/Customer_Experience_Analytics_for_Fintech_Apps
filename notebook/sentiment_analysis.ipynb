{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b9fb74-6e5c-4562-8610-e6cb6a1d20e0",
   "metadata": {},
   "source": [
    "# Task 2: Sentiment and Thematic Analysis\n",
    "**Quantify review sentiment and identify themes to uncover satisfaction drivers and pain points**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Load Your Clean Data](#load-data)\n",
    "3. [Sentiment Analysis](#sentiment)\n",
    "4. [Bank-Wise Analysis](#bank-analysis)\n",
    "5. [Thematic Analysis](#thematic)\n",
    "6. [Visualizations](#visualizations)\n",
    "7. [Export Results](#export)\n",
    "8. [KPI Validation](#kpi)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "### Install Required Packages\n",
    "Run this cell only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a47578-9a81-4c5e-aa9e-e35f761a9546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "‚úÖ Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy matplotlib seaborn nltk textblob scikit-learn -q\n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26068dc8-24cc-47b0-a523-cddb2ef8f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ermias/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ermias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ermias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"‚úÖ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96213fb9-fd14-4e42-a395-549ad636ff62",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "640d8cc4-3a8a-4482-9441-d6024648c1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Advanced NLP (optional)\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è spaCy not available. Some features may be limited.\")\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# System\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "833a82b8-b8e2-4888-91cf-a60f597263bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.configLoader import config_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a57f662-760d-4319-97de-e0a8c133ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/reviews_processed.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_processed = config_loader.load_config('scraping')['base_processing']['paths']['processed_reviews']\n",
    "path_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35fe96ee-a341-4f68-89ab-d45b61098d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_year</th>\n",
       "      <th>review_month</th>\n",
       "      <th>bank_code</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>thumbs_up</th>\n",
       "      <th>text_length</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3463230e-f9f7-4be3-a632-fdd8d017ce84</td>\n",
       "      <td>üôèüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>2025</td>\n",
       "      <td>11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Bank of Abyssinia</td>\n",
       "      <td>Yasin Alemu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a6cbfa34-f2b1-4a16-96b6-c94f58cea76f</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025</td>\n",
       "      <td>11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Bank of Abyssinia</td>\n",
       "      <td>Wariyo Dida</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fc67d12c-92e2-45aa-a9e0-011f58a583bc</td>\n",
       "      <td>goof</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025</td>\n",
       "      <td>11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Bank of Abyssinia</td>\n",
       "      <td>Hailegebrail Tegegn</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11306fb9-5571-4950-8d32-604c5402242f</td>\n",
       "      <td>good!</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>2025</td>\n",
       "      <td>11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Bank of Abyssinia</td>\n",
       "      <td>Tsegay ab</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>809c46d2-730e-446a-9061-2a45e978ad9d</td>\n",
       "      <td>good jop</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>2025</td>\n",
       "      <td>11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Bank of Abyssinia</td>\n",
       "      <td>Yohanis Fikadu</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>b3c8405c-96a7-4b5e-884c-76c97c530c34</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>Dashen</td>\n",
       "      <td>Dashen Bank</td>\n",
       "      <td>fenta abebayehu</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>e1c1214a-8bc1-45db-bc49-3d51dddc6b88</td>\n",
       "      <td>Amazing app super easy to use and best design....</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>Dashen</td>\n",
       "      <td>Dashen Bank</td>\n",
       "      <td>Yitbarek Gossaye</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>8d1d472b-2bae-4749-b089-5632108ade02</td>\n",
       "      <td>its the best ever</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>Dashen</td>\n",
       "      <td>Dashen Bank</td>\n",
       "      <td>natnael abera</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>749851ed-72d7-4f1c-8e5e-27dd822b5008</td>\n",
       "      <td>nice</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>Dashen</td>\n",
       "      <td>Dashen Bank</td>\n",
       "      <td>Mehammed Amin</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>f2852962-b983-44af-b296-241a3373477f</td>\n",
       "      <td>excellent game changer App</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>Dashen</td>\n",
       "      <td>Dashen Bank</td>\n",
       "      <td>Wondim Tarko</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 review_id  \\\n",
       "0     3463230e-f9f7-4be3-a632-fdd8d017ce84   \n",
       "1     a6cbfa34-f2b1-4a16-96b6-c94f58cea76f   \n",
       "2     fc67d12c-92e2-45aa-a9e0-011f58a583bc   \n",
       "3     11306fb9-5571-4950-8d32-604c5402242f   \n",
       "4     809c46d2-730e-446a-9061-2a45e978ad9d   \n",
       "...                                    ...   \n",
       "1195  b3c8405c-96a7-4b5e-884c-76c97c530c34   \n",
       "1196  e1c1214a-8bc1-45db-bc49-3d51dddc6b88   \n",
       "1197  8d1d472b-2bae-4749-b089-5632108ade02   \n",
       "1198  749851ed-72d7-4f1c-8e5e-27dd822b5008   \n",
       "1199  f2852962-b983-44af-b296-241a3373477f   \n",
       "\n",
       "                                            review_text  rating review_date  \\\n",
       "0                                                    üôèüëç       5  2025-11-29   \n",
       "1                                             Very Good       5  2025-11-28   \n",
       "2                                                  goof       5  2025-11-28   \n",
       "3                                                 good!       5  2025-11-28   \n",
       "4                                              good jop       5  2025-11-27   \n",
       "...                                                 ...     ...         ...   \n",
       "1195                                               good       5  2025-05-09   \n",
       "1196  Amazing app super easy to use and best design....       5  2025-05-09   \n",
       "1197                                  its the best ever       5  2025-05-09   \n",
       "1198                                               nice       5  2025-05-08   \n",
       "1199                         excellent game changer App       5  2025-05-08   \n",
       "\n",
       "      review_year  review_month bank_code          bank_name  \\\n",
       "0            2025            11       BOA  Bank of Abyssinia   \n",
       "1            2025            11       BOA  Bank of Abyssinia   \n",
       "2            2025            11       BOA  Bank of Abyssinia   \n",
       "3            2025            11       BOA  Bank of Abyssinia   \n",
       "4            2025            11       BOA  Bank of Abyssinia   \n",
       "...           ...           ...       ...                ...   \n",
       "1195         2025             5    Dashen        Dashen Bank   \n",
       "1196         2025             5    Dashen        Dashen Bank   \n",
       "1197         2025             5    Dashen        Dashen Bank   \n",
       "1198         2025             5    Dashen        Dashen Bank   \n",
       "1199         2025             5    Dashen        Dashen Bank   \n",
       "\n",
       "                user_name  thumbs_up  text_length       source  \n",
       "0             Yasin Alemu          0            2  Google Play  \n",
       "1             Wariyo Dida          0            9  Google Play  \n",
       "2     Hailegebrail Tegegn          0            4  Google Play  \n",
       "3               Tsegay ab          0            5  Google Play  \n",
       "4          Yohanis Fikadu          0            8  Google Play  \n",
       "...                   ...        ...          ...          ...  \n",
       "1195      fenta abebayehu          1            4  Google Play  \n",
       "1196     Yitbarek Gossaye          1           56  Google Play  \n",
       "1197        natnael abera          1           17  Google Play  \n",
       "1198        Mehammed Amin          1            4  Google Play  \n",
       "1199         Wondim Tarko          1           26  Google Play  \n",
       "\n",
       "[1200 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path_processed)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04378db3-a113-4aa3-9a98-3c7ce79e94a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_id', 'review_text', 'rating', 'review_date', 'review_year',\n",
       "       'review_month', 'bank_code', 'bank_name', 'user_name', 'thumbs_up',\n",
       "       'text_length', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77a7dc98-386a-416a-a7cc-08376bcb9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f609c337-6763-4e2e-96e9-9d1686e54c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    score = sia.polarity_scores(str(text))['compound']\n",
    "    if score >= 0.05:\n",
    "        return 'positive', score\n",
    "    elif score <= -0.05:\n",
    "        return 'negative', score\n",
    "    else:\n",
    "        return 'neutral', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc7b97a-eb36-45f9-88dc-35db9885684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: reviews_with_sentiment.csv\n",
      "\n",
      "üìä BANK SENTIMENT TOTALS:\n",
      "Bank of Abyssinia: 400 reviews, 187 positive (47%)\n",
      "Commercial Bank of Ethiopia: 400 reviews, 230 positive (57%)\n",
      "Dashen Bank: 400 reviews, 256 positive (64%)\n"
     ]
    }
   ],
   "source": [
    "df['sentiment_label'], df['sentiment_score'] = zip(*df['review_text'].apply(get_sentiment))\n",
    "\n",
    "# 4. Save results\n",
    "df.to_csv('reviews_with_sentiment.csv', index=False)\n",
    "print(\"‚úÖ Saved: reviews_with_sentiment.csv\")\n",
    "a\n",
    "# 5. Show bank totals\n",
    "print(\"\\nüìä BANK SENTIMENT TOTALS:\")\n",
    "for bank in df['bank_name'].unique():\n",
    "    bank_data = df[df['bank_name'] == bank]\n",
    "    total = len(bank_data)\n",
    "    pos = (bank_data['sentiment_label'] == 'positive').sum()\n",
    "    print(f\"{bank}: {total} reviews, {pos} positive ({pos/total*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f63361d5-96b2-4a09-8fe7-0942104667b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BY RATING:\n",
      "----------\n",
      "1-star: 263 reviews, avg sentiment = -0.15\n",
      "2-star: 46 reviews, avg sentiment = 0.05\n",
      "3-star: 67 reviews, avg sentiment = 0.17\n",
      "4-star: 88 reviews, avg sentiment = 0.33\n",
      "5-star: 736 reviews, avg sentiment = 0.38\n"
     ]
    }
   ],
   "source": [
    "# 2. AGGREGATE BY RATING  \n",
    "print(\"\\nBY RATING:\")\n",
    "print(\"----------\")\n",
    "for rating in sorted(df['rating'].unique()):\n",
    "    rating_data = df[df['rating'] == rating]\n",
    "    avg_sentiment = rating_data['sentiment_score'].mean()\n",
    "    count = len(rating_data)\n",
    "    print(f\"{rating}-star: {count} reviews, avg sentiment = {avg_sentiment:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72aa7917-cc5f-46ba-a4c6-11a2db072d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BANK REVIEW THEMATIC ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. Loading and preprocessing data...\n",
      "   Loaded 1200 reviews\n",
      "\n",
      "2. Extracting keywords using TF-IDF...\n",
      "\n",
      "   Top 20 Keywords by TF-IDF Score:\n",
      "   ----------------------------------------\n",
      "   59. good                           (Score: 156.3678)\n",
      "    8. app                            (Score: 121.1389)\n",
      "   20. best                           (Score: 58.6278)\n",
      "   92. nice                           (Score: 46.8196)\n",
      "   14. bank                           (Score: 40.6634)\n",
      "   74. like                           (Score: 23.4649)\n",
      "   150. wow                            (Score: 22.8846)\n",
      "   52. excellent                      (Score: 22.6363)\n",
      "   17. banking                        (Score: 22.2009)\n",
      "   60. good app                       (Score: 20.5360)\n",
      "   137. use                            (Score: 20.2597)\n",
      "   10. application                    (Score: 19.0527)\n",
      "   146. working                        (Score: 18.4348)\n",
      "   61. great                          (Score: 18.3925)\n",
      "   54. fast                           (Score: 17.8575)\n",
      "   145. work                           (Score: 17.0365)\n",
      "   83. mobile                         (Score: 16.5916)\n",
      "   134. update                         (Score: 16.4690)\n",
      "   21. best app                       (Score: 16.1313)\n",
      "   27. cbe                            (Score: 16.0254)\n",
      "\n",
      "3. Extracting banking entities with spaCy...\n",
      "\n",
      "   Top Entities Found:\n",
      "   ----------------------------------------\n",
      "   CARDINAL: dozen, 30, one\n",
      "   ORG: bank of abyssinia address, hussen hassen umer, alhamdulilah, u develop bank apps, zedo\n",
      "\n",
      "4. Clustering keywords into themes...\n",
      "\n",
      "   Themes Identified (with keyword counts):\n",
      "   ----------------------------------------\n",
      "   Other                    : 36 keywords\n",
      "     Sample: good, best, nice, bank, like\n",
      "   User Interface Experience: 10 keywords\n",
      "     Sample: app, good app, application, update, best app\n",
      "   Transaction Performance  :  4 keywords\n",
      "     Sample: fast, slow, transfer, transaction\n",
      "\n",
      "5. Applying topic modeling (LDA) for validation...\n",
      "\n",
      "   LDA Topics (Top Keywords):\n",
      "   ----------------------------------------\n",
      "   Topic 1: app, working, excellent, doesn, work\n",
      "   Topic 2: good, app, nice, use, easy\n",
      "   Topic 3: best, step, ·äê·ãç, ethiopia, ok\n",
      "   Topic 4: app, banking, bank, dashen, super\n",
      "   Topic 5: app, application, great, time, fast\n",
      "\n",
      "============================================================\n",
      "ACTIONABLE INSIGHTS & RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "User Interface Experience:\n",
      "  Prevalence: 83.2% of significant feedback\n",
      "  Key Issues: app, good app, application, update, best app\n",
      "  Recommendation: Update app UI and fix crashing issues\n",
      "\n",
      "Transaction Performance:\n",
      "  Prevalence: 16.8% of significant feedback\n",
      "  Key Issues: fast, slow, transfer, transaction\n",
      "  Recommendation: Optimize transfer speeds and clear pending statuses\n",
      "\n",
      "Results exported to: thematic_analysis_results.csv\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Review the exported CSV file for detailed insights\n",
      "2. Validate themes with business stakeholders\n",
      "3. Prioritize issues based on prevalence and impact\n",
      "4. Track improvements with sentiment analysis over time\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# COMPLETE THEMATIC ANALYSIS WORKFLOW\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# -------------------------------------------------\n",
    "def load_and_preprocess_data(filepath='bank_reviews.csv'):\n",
    "    \"\"\"\n",
    "    Load review data and preprocess text\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    reviews = df\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    reviews['cleaned_text'] = reviews['review_text'].fillna('').astype(str)\n",
    "    \n",
    "    # Remove special characters but keep important punctuation for n-grams\n",
    "    reviews['cleaned_text'] = reviews['cleaned_text'].apply(\n",
    "        lambda x: re.sub(r'[^\\w\\s\\-\\.@#]', ' ', x.lower())\n",
    "    )\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    reviews['cleaned_text'] = reviews['cleaned_text'].apply(\n",
    "        lambda x: ' '.join(x.split())\n",
    "    )\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. KEYWORD EXTRACTION WITH TF-IDF\n",
    "# -------------------------------------------------\n",
    "def extract_keywords_tfidf(reviews, text_column='cleaned_text', max_features=150):\n",
    "    \"\"\"\n",
    "    Extract significant keywords using TF-IDF\n",
    "    \"\"\"\n",
    "    # Initialize TF-IDF Vectorizer with banking-specific stop words\n",
    "    banking_stop_words = ['bank', 'account', 'money', 'would', 'like', 'get', 'one', 'use']\n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 3),          # Extract unigrams, bigrams, and trigrams\n",
    "        min_df=2,                    # Minimum document frequency\n",
    "        max_df=0.8,                  # Maximum document frequency (avoid too common)\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\b[a-zA-Z]{3,}\\b',  # Words with at least 3 characters\n",
    "        vocabulary=None\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    tfidf_matrix = tfidf.fit_transform(reviews[text_column])\n",
    "    keywords = tfidf.get_feature_names_out()\n",
    "    tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Create keyword DataFrame with scores\n",
    "    keyword_df = pd.DataFrame({\n",
    "        'keyword': keywords,\n",
    "        'tfidf_score': tfidf_scores,\n",
    "        'frequency': np.array(tfidf_matrix.astype(bool).sum(axis=0)).flatten()\n",
    "    }).sort_values('tfidf_score', ascending=False)\n",
    "    \n",
    "    # Get top keywords by score\n",
    "    top_keywords = keyword_df.head(50)\n",
    "    \n",
    "    return tfidf, tfidf_matrix, keyword_df, top_keywords\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. ENHANCE WITH spaCy NAMED ENTITY RECOGNITION\n",
    "# -------------------------------------------------\n",
    "def extract_entities_spacy(reviews, text_column='cleaned_text'):\n",
    "    \"\"\"\n",
    "    Extract banking-specific entities using spaCy\n",
    "    \"\"\"\n",
    "    # Load spaCy model\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        # If model not installed, install it first\n",
    "        import subprocess\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # Process each review\n",
    "    for text in reviews[text_column].head(100):  # Process first 100 for efficiency\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['ORG', 'PRODUCT', 'MONEY', 'CARDINAL']:\n",
    "                entities[ent.label_].append(ent.text.lower())\n",
    "    \n",
    "    # Count entity frequencies\n",
    "    entity_counts = {}\n",
    "    for label, texts in entities.items():\n",
    "        entity_counts[label] = pd.Series(texts).value_counts().head(10)\n",
    "    \n",
    "    return entity_counts\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. MANUAL/RULE-BASED THEME CLUSTERING\n",
    "# -------------------------------------------------\n",
    "def cluster_keywords_into_themes(keyword_df, top_n=50):\n",
    "    \"\"\"\n",
    "    Manually cluster keywords into 3-5 banking themes\n",
    "    \"\"\"\n",
    "    # Get top keywords for clustering\n",
    "    top_keywords = keyword_df.nlargest(top_n, 'tfidf_score')['keyword'].tolist()\n",
    "    \n",
    "    # Define theme categories with pattern matching\n",
    "    theme_patterns = {\n",
    "        'ACCOUNT_ACCESS_SECURITY': [\n",
    "            r'login', r'password', r'access', r'security', r'authenticat', \n",
    "            r'locked', r'reset', r'biometric', r'face id', r'fingerprint',\n",
    "            r'two factor', r'verification', r'security code', r'cant login'\n",
    "        ],\n",
    "        'TRANSACTION_PERFORMANCE': [\n",
    "            r'transfer', r'transaction', r'payment', r'slow', r'fast',\n",
    "            r'instant', r'pending', r'failed', r'processed', r'wire',\n",
    "            r'direct deposit', r'mobile deposit', r'check deposit',\n",
    "            r'money transfer', r'payment processing'\n",
    "        ],\n",
    "        'USER_INTERFACE_EXPERIENCE': [\n",
    "            r'app', r'ui', r'ux', r'interface', r'design',\n",
    "            r'easy to use', r'intuitive', r'navigation', r'crash',\n",
    "            r'bug', r'glitch', r'loading', r'slow app', r'update',\n",
    "            r'mobile banking', r'dashboard', r'layout'\n",
    "        ],\n",
    "        'CUSTOMER_SUPPORT': [\n",
    "            r'customer service', r'support', r'help', r'call',\n",
    "            r'wait', r'chat', r'representative', r'agent',\n",
    "            r'response', r'complaint', r'issue resolved',\n",
    "            r'phone support', r'live chat', r'escalation'\n",
    "        ],\n",
    "        'FEES_FINANCIAL_FEATURES': [\n",
    "            r'fee', r'charge', r'cost', r'overdraft',\n",
    "            r'interest', r'rate', r'apr', r'reward',\n",
    "            r'cashback', r'credit card', r'loan',\n",
    "            r'savings', r'checking', r'monthly fee',\n",
    "            r'atm fee', r'foreign transaction'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Cluster keywords into themes\n",
    "    theme_keywords = defaultdict(list)\n",
    "    theme_scores = defaultdict(float)\n",
    "    \n",
    "    for keyword in top_keywords:\n",
    "        keyword_lower = keyword.lower()\n",
    "        assigned = False\n",
    "        \n",
    "        for theme, patterns in theme_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, keyword_lower, re.IGNORECASE):\n",
    "                    theme_keywords[theme].append(keyword)\n",
    "                    \n",
    "                    # Get TF-IDF score for this keyword\n",
    "                    kw_score = keyword_df[keyword_df['keyword'] == keyword]['tfidf_score'].values\n",
    "                    if len(kw_score) > 0:\n",
    "                        theme_scores[theme] += kw_score[0]\n",
    "                    \n",
    "                    assigned = True\n",
    "                    break\n",
    "            \n",
    "            if assigned:\n",
    "                break\n",
    "        \n",
    "        # If no theme matched, add to \"OTHER\"\n",
    "        if not assigned:\n",
    "            theme_keywords['OTHER'].append(keyword)\n",
    "    \n",
    "    return theme_keywords, theme_scores, theme_patterns\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. TOPIC MODELING (OPTIONAL ENHANCEMENT)\n",
    "# -------------------------------------------------\n",
    "def apply_topic_modeling(reviews, text_column='cleaned_text', n_topics=5):\n",
    "    \"\"\"\n",
    "    Apply LDA topic modeling for theme validation\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2\n",
    "    )\n",
    "    \n",
    "    dtm = vectorizer.fit_transform(reviews[text_column])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Apply LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=10,\n",
    "        learning_method='online'\n",
    "    )\n",
    "    \n",
    "    lda_topics = lda.fit_transform(dtm)\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[-10:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topic_keywords.append(top_features)\n",
    "    \n",
    "    return lda, topic_keywords, lda_topics\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. MAIN EXECUTION PIPELINE\n",
    "# -------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Complete thematic analysis pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BANK REVIEW THEMATIC ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"\\n1. Loading and preprocessing data...\")\n",
    "    reviews = load_and_preprocess_data('bank_reviews.csv')\n",
    "    print(f\"   Loaded {len(reviews)} reviews\")\n",
    "    \n",
    "    # Step 2: Extract keywords with TF-IDF\n",
    "    print(\"\\n2. Extracting keywords using TF-IDF...\")\n",
    "    tfidf, tfidf_matrix, keyword_df, top_keywords = extract_keywords_tfidf(reviews)\n",
    "    \n",
    "    print(\"\\n   Top 20 Keywords by TF-IDF Score:\")\n",
    "    print(\"   \" + \"-\" * 40)\n",
    "    for idx, row in top_keywords.head(20).iterrows():\n",
    "        print(f\"   {idx+1:2d}. {row['keyword']:30s} (Score: {row['tfidf_score']:.4f})\")\n",
    "    \n",
    "    # Step 3: Extract entities with spaCy\n",
    "    print(\"\\n3. Extracting banking entities with spaCy...\")\n",
    "    entity_counts = extract_entities_spacy(reviews)\n",
    "    \n",
    "    print(\"\\n   Top Entities Found:\")\n",
    "    print(\"   \" + \"-\" * 40)\n",
    "    for label, counts in entity_counts.items():\n",
    "        print(f\"   {label}: {', '.join(counts.head(5).index.tolist())}\")\n",
    "    \n",
    "    # Step 4: Cluster into themes\n",
    "    print(\"\\n4. Clustering keywords into themes...\")\n",
    "    theme_keywords, theme_scores, theme_patterns = cluster_keywords_into_themes(keyword_df)\n",
    "    \n",
    "    print(\"\\n   Themes Identified (with keyword counts):\")\n",
    "    print(\"   \" + \"-\" * 40)\n",
    "    for theme, keywords in theme_keywords.items():\n",
    "        if keywords:  # Only show themes with keywords\n",
    "            print(f\"   {theme.replace('_', ' ').title():25s}: {len(keywords):2d} keywords\")\n",
    "            if len(keywords) > 0:\n",
    "                print(f\"     Sample: {', '.join(keywords[:5])}\")\n",
    "    \n",
    "    # Step 5: Optional topic modeling\n",
    "    print(\"\\n5. Applying topic modeling (LDA) for validation...\")\n",
    "    try:\n",
    "        lda, topic_keywords, lda_topics = apply_topic_modeling(reviews, n_topics=5)\n",
    "        \n",
    "        print(\"\\n   LDA Topics (Top Keywords):\")\n",
    "        print(\"   \" + \"-\" * 40)\n",
    "        for i, keywords in enumerate(topic_keywords):\n",
    "            print(f\"   Topic {i+1}: {', '.join(keywords[:5])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Topic modeling skipped: {e}\")\n",
    "    \n",
    "    # Step 6: Generate actionable insights\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ACTIONABLE INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate theme prevalence\n",
    "    total_score = sum(theme_scores.values())\n",
    "    \n",
    "    for theme, score in sorted(theme_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        if score > 0:\n",
    "            prevalence = (score / total_score) * 100\n",
    "            keywords = theme_keywords.get(theme, [])\n",
    "            \n",
    "            print(f\"\\n{theme.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Prevalence: {prevalence:.1f}% of significant feedback\")\n",
    "            print(f\"  Key Issues: {', '.join(keywords[:5])}\")\n",
    "            \n",
    "            # Generate recommendations\n",
    "            if 'ACCESS' in theme:\n",
    "                print(\"  Recommendation: Improve login flow and add biometric options\")\n",
    "            elif 'TRANSACTION' in theme:\n",
    "                print(\"  Recommendation: Optimize transfer speeds and clear pending statuses\")\n",
    "            elif 'INTERFACE' in theme:\n",
    "                print(\"  Recommendation: Update app UI and fix crashing issues\")\n",
    "            elif 'SUPPORT' in theme:\n",
    "                print(\"  Recommendation: Reduce wait times and train support staff\")\n",
    "            elif 'FEES' in theme:\n",
    "                print(\"  Recommendation: Review fee structure and offer fee-free options\")\n",
    "    \n",
    "    # Return analysis results\n",
    "    results = {\n",
    "        'reviews': reviews,\n",
    "        'keyword_df': keyword_df,\n",
    "        'theme_keywords': dict(theme_keywords),\n",
    "        'theme_scores': dict(theme_scores),\n",
    "        'entity_counts': entity_counts\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. EXPORT RESULTS\n",
    "# -------------------------------------------------\n",
    "def export_results(results, output_file='thematic_analysis_results.csv'):\n",
    "    \"\"\"\n",
    "    Export analysis results to CSV\n",
    "    \"\"\"\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for theme, keywords in results['theme_keywords'].items():\n",
    "        if keywords:\n",
    "            for keyword in keywords:\n",
    "                # Get keyword info\n",
    "                kw_info = results['keyword_df'][results['keyword_df']['keyword'] == keyword]\n",
    "                if not kw_info.empty:\n",
    "                    tfidf_score = kw_info['tfidf_score'].values[0]\n",
    "                    frequency = kw_info['frequency'].values[0]\n",
    "                    \n",
    "                    summary_data.append({\n",
    "                        'theme': theme.replace('_', ' ').title(),\n",
    "                        'keyword': keyword,\n",
    "                        'tfidf_score': tfidf_score,\n",
    "                        'frequency': frequency,\n",
    "                        'prevalence_percentage': (tfidf_score / results['keyword_df']['tfidf_score'].sum()) * 100\n",
    "                    })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults exported to: {output_file}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# EXECUTE THE ANALYSIS\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete analysis\n",
    "    analysis_results = main()\n",
    "    \n",
    "    # Export results\n",
    "    export_results(analysis_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Review the exported CSV file for detailed insights\")\n",
    "    print(\"2. Validate themes with business stakeholders\")\n",
    "    print(\"3. Prioritize issues based on prevalence and impact\")\n",
    "    print(\"4. Track improvements with sentiment analysis over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f77587-cc30-429d-9c4c-0b19d58fc82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
